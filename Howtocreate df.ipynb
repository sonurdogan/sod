{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession,types\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.master(\"local\").appName(\"rddtodf\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Category A', 100, \"This is category A\"),\n",
    "        ('Category B', 120, \"This is category B\"),\n",
    "        ('Category C', 150, \"This is category C\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olan bir datadan önce sütunları belirterek df oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('Category', StringType(), True),\n",
    "    StructField('Count', IntegerType(), True),\n",
    "    StructField('Description', StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|  Category|Count|       Description|\n",
      "+----------+-----+------------------+\n",
      "|Category A|  100|This is category A|\n",
      "|Category B|  120|This is category B|\n",
      "|Category C|  150|This is category C|\n",
      "+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "df = spark.createDataFrame(rdd,schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|  Category|Count|       Description|\n",
      "+----------+-----+------------------+\n",
      "|Category A|  100|This is category A|\n",
      "|Category B|  120|This is category B|\n",
      "|Category C|  150|This is category C|\n",
      "+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=rdd.toDF(schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Column Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([(1,2,3),(4,5,6),(7,8,9)])\n",
    "df2= rdd2.toDF([\"a\",\"b\",\"c\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix>>List>>df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003170113177261, 0.9294842368846181, 0.5951352194437769, 0.9464356827662265], [0.7687053402592972, 0.007480813116247154, 0.46750605930750755, 0.3488848255087381], [0.20206086915746158, 0.7434224316119984, 0.41958594820891437, 0.7455660000904909], [0.8593748672263974, 0.730358327253481, 0.6795836316151267, 0.7009543426810341]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(4,4)\n",
    "y = x.tolist()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=sc.parallelize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= rdd3.toDF([\"a\",\"b\",\"c\",\"d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------------+------------------+\n",
      "|                  a|                   b|                  c|                 d|\n",
      "+-------------------+--------------------+-------------------+------------------+\n",
      "|0.09003170113177261|  0.9294842368846181| 0.5951352194437769|0.9464356827662265|\n",
      "| 0.7687053402592972|0.007480813116247154|0.46750605930750755|0.3488848255087381|\n",
      "|0.20206086915746158|  0.7434224316119984|0.41958594820891437|0.7455660000904909|\n",
      "| 0.8593748672263974|   0.730358327253481| 0.6795836316151267|0.7009543426810341|\n",
      "+-------------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
